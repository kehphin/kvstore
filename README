This project took a lot of planning - we approached the problem in small pieces to not get overwhelmed. Overall, we met up about 4 times and partner programmed, bouncing ideas off each other and catching each other's mistakes. We also each worked on the project independently as well in between these coding sessions. For testing, we used a ton of print lines to make sure that heartbeats and elections were working correctly, and that each replica had the correct data in its dictionary. 

At a high level, we followed the RAFT paper's implementation fairly closely, taking advantage of the python starter code provided to us. 
First, we hardcoded a leader and followers, and made sure that our heartbeats could be read and responded to by each replica. This was key because the heartbeat is the key part of communication between nodes. 

Then, we added the ability to redirect and receive get() and put() messages, without actually storing the data.

After this, we implemented the actual leader election - this was tough, especially when our election packets were dropped. We made sure to change the term when a new leader was elected, and also randomize the timeout between 100 and 300 ms. There were a lot of nuances to establish a quorum, which included setting a quorum timeout, as well as comparing the length of each replica's log in order to select the leader that had the longest log. Ultimately, we ended up adding in extra logic to speed up the election process. When a replica timed out, it would start an election as described in the Raft paper. However, most of the replicas would time out at the same time, so they would all vote for themselves and no one would be elected. In the Raft paper, they handled this situation by adding randomized timeouts. However, this left a lot up to timing, and was especially clunky when messages were being dropped at a high rate. So, we added extra logic to remove this. During the election process, each replica will continue listening for the other's election messages. After hearing messages from all other replicas, we can determine which replica we should elect (the one with the highest commit number). If the first election at a replica times out, we only start a new one if we have the highest commit number. Otherwise, we just wait to hear the election message from the leader who does have the highest commit number, and then we vote for them. This way, the election process is finished after at most 2 election timeouts. 

With the leader in place, and the functionality to receive client requests, we finally implemented the append messages that would inform a follower which entries to append in its data dictionary (state machine). Just as in Raft, each append message would include new messages to append to the replica's logs, and commit information so that replicas knew when it was safe to commit entries.

The last thing we had to do was add some logic to save the messages that arrived when we were in the process of an election. We couldn't rerdirect them to the leader, because we had no leader. So, we have to store the messages and then process them after we have re-elected a new leader. Then, whenever a replica responds to a leader's append message, they include their message queue (in most cases the queue is an empty list). In return, the leader uses its append message to notify the replicas that it has received their message queue, at which point the replica can clear it. Only once a newly elected leader has received the message queue of each replica, and has processed each of these client requests (in order), can they continue processing new incoming client requests. This ensures that no messages are lost along the way, and that our key value pairs are always updated in the correct order. 

Following this, it was just a matter of going through all the tests one by one, and making small fixes for corner cases until we passed it.

On the day that the project was due when we were running our tests for the last time, we began encountering weird errors from the testing script. The errors happened during different test cases each time, and it seemed like they were being caused by socket issues. We then logged onto cs3600tcp.ccs.neu.edu and reran the test script and it worked fine. It also worked fine locally. This led us to believe that it was just some weird problem with login.ccs.neu.edu, because it consistently failed when we ran it there, but it didnt once fail when we ran it multiple times on cs3600tcp.ccs.neu.edu. Further, each test passes when you run it with ./run.py config-file, so we don't think that this was an error on our end. 